
Running experiments on BasicGridWorld:
Pr(succ) = 0.75
[0,1,0,0]
[0,0,0,0]
[0,1,0,0]
[0,0,0,0]

Max iterations: 100, max intervals: 100
//Value Iteration Analysis//
Value Iteration,17,11,6,6,8,12,10,11,10,9,8,9,11,10,11,8,8,6,7,14,12,11,8,13,16,8,10,10,11,16,14,13,11,6,11,12,15,9,8,10,23,13,11,20,10,18,10,11,23,9,6,12,15,11,10,19,8,8,11,7,14,9,12,10,10,10,11,9,11,6,6,15,11,23,8,9,12,8,14,10,18,8,9,9,7,11,9,9,9,10,8,8,8,15,8,8,15,20,15,8

This is your optimal policy:
[v,*,>,>]
[>,>,^,^]
[^,*,^,^]
[^,<,>,^]

//Policy Iteration Analysis//
Policy Iteration,14,12,10,8,9,9,10,10,8,10,6,13,11,10,13,10,9,9,11,13,10,10,16,11,13,13,8,12,19,8,12,11,10,10,8,10,14,13,11,9,10,13,8,21,10,9,9,18,8,15,11,13,19,9,8,8,13,11,9,9,10,6,8,8,9,11,8,10,13,8,8,12,10,9,8,9,12,8,12,19,9,9,9,8,9,11,14,13,19,10,10,10,11,15,6,11,14,9,9,12

This is your optimal policy:
[v,*,>,<]
[>,>,^,^]
[^,*,^,^]
[^,<,>,^]

//Q Learning Analysis//
Q Learning,38,13,44,33,9,29,14,14,14,120,13,7,23,9,8,15,10,108,18,17,11,16,16,120,72,19,8,27,25,37,11,12,10,8,7,18,30,12,8,17,27,8,11,13,19,16,14,8,81,26,38,20,9,26,6,12,84,11,12,75,11,8,8,10,14,13,18,14,18,8,138,41,15,16,53,12,12,18,18,15,11,91,18,9,19,23,6,10,8,9,8,10,9,9,10,18,71,11,59,47

This is your optimal policy:
[v,*,>,>]
[>,<,^,^]
[^,*,>,v]
[v,<,<,v]

//Aggregate Analysis//

The data below shows the number of passes the agent required to converge
on the optimal policy
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,1,2,3,4,5,6,7,8,9,10,11,12,13,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14
Policy Iteration,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15
Q Learning,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,17,11,6,6,8,12,10,11,10,9,8,9,11,10,11,8,8,6,7,14,12,11,8,13,16,8,10,10,11,16,14,13,11,6,11,12,15,9,8,10,23,13,11,20,10,18,10,11,23,9,6,12,15,11,10,19,8,8,11,7,14,9,12,10,10,10,11,9,11,6,6,15,11,23,8,9,12,8,14,10,18,8,9,9,7,11,9,9,9,10,8,8,8,15,8,8,15,20,15,8
Policy Iteration,14,12,10,8,9,9,10,10,8,10,6,13,11,10,13,10,9,9,11,13,10,10,16,11,13,13,8,12,19,8,12,11,10,10,8,10,14,13,11,9,10,13,8,21,10,9,9,18,8,15,11,13,19,9,8,8,13,11,9,9,10,6,8,8,9,11,8,10,13,8,8,12,10,9,8,9,12,8,12,19,9,9,9,8,9,11,14,13,19,10,10,10,11,15,6,11,14,9,9,12
Q Learning,38,13,44,33,9,29,14,14,14,120,13,7,23,9,8,15,10,108,18,17,11,16,16,120,72,19,8,27,25,37,11,12,10,8,7,18,30,12,8,17,27,8,11,13,19,16,14,8,81,26,38,20,9,26,6,12,84,11,12,75,11,8,8,10,14,13,18,14,18,8,138,41,15,16,53,12,12,18,18,15,11,91,18,9,19,23,6,10,8,9,8,10,9,9,10,18,71,11,59,47

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,67,24,23,11,14,17,8,8,7,7,6,6,6,17,5,4,4,3,3,3,3,4,4,3,3,10,4,6,4,3,5,5,3,3,4,4,4,3,7,3,3,3,3,3,3,3,4,3,3,3,8,3,3,3,3,3,3,3,3,3,3,3,3,2,3,2,2,5,4,5,5,4,4,4,4,4,8,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,2,2,2,2,2,2,2
Policy Iteration,2,2,2,2,3,5,4,4,6,7,7,7,6,8,12,10,5,7,8,7,10,14,11,10,10,10,10,11,12,10,16,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,6,5,6,7,5,13,5,5,5,5,5,5,5,5,5,5,5,5,5,5,6,5,5,5,5,5,5,5,5,5,5,6,8,8,10,10,9,9,9,10,10,14,10,10,11,11,13,10,9,9
Q Learning,3,2,3,4,5,4,3,7,7,11,9,5,5,5,6,20,3,12,2,5,6,3,4,4,4,2,4,5,7,7,10,3,3,3,4,3,4,4,12,3,9,6,6,4,9,5,8,6,3,5,4,8,6,13,9,11,9,9,6,6,6,16,5,4,6,7,8,5,5,16,6,5,8,7,15,5,12,7,17,8,11,9,7,11,11,8,8,7,16,9,16,33,15,26,21,38,31,20,23,22

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration Rewards,85.0,91.0,96.0,96.0,-5.0,-9.0,92.0,91.0,92.0,93.0,94.0,93.0,91.0,92.0,91.0,94.0,94.0,96.0,95.0,88.0,90.0,91.0,94.0,89.0,86.0,94.0,92.0,92.0,91.0,86.0,-11.0,89.0,91.0,96.0,91.0,-9.0,87.0,93.0,94.0,-7.0,79.0,-10.0,91.0,-17.0,92.0,-114.0,92.0,91.0,-218.0,93.0,96.0,90.0,87.0,-8.0,92.0,-214.0,94.0,94.0,91.0,-4.0,88.0,93.0,-9.0,92.0,-7.0,92.0,91.0,93.0,91.0,96.0,96.0,87.0,91.0,-119.0,94.0,93.0,90.0,94.0,88.0,92.0,84.0,94.0,93.0,93.0,-4.0,91.0,93.0,93.0,93.0,92.0,94.0,94.0,94.0,-111.0,94.0,94.0,87.0,-116.0,87.0,94.0
Policy Iteration Rewards,88.0,90.0,92.0,94.0,93.0,93.0,92.0,92.0,94.0,92.0,-3.0,-109.0,91.0,92.0,89.0,92.0,93.0,93.0,91.0,89.0,92.0,92.0,86.0,91.0,89.0,-10.0,94.0,90.0,83.0,94.0,90.0,91.0,92.0,92.0,94.0,92.0,-11.0,89.0,91.0,93.0,92.0,-10.0,94.0,81.0,-7.0,93.0,93.0,-15.0,94.0,87.0,91.0,89.0,83.0,93.0,94.0,94.0,89.0,-107.0,93.0,93.0,-7.0,96.0,94.0,94.0,93.0,91.0,94.0,92.0,89.0,94.0,94.0,-9.0,92.0,93.0,94.0,93.0,90.0,94.0,90.0,83.0,93.0,93.0,93.0,94.0,93.0,91.0,88.0,89.0,-16.0,92.0,92.0,92.0,91.0,87.0,96.0,91.0,88.0,93.0,93.0,90.0
Q Learning Rewards,64.0,-109.0,58.0,-30.0,93.0,73.0,88.0,88.0,88.0,-18.0,89.0,95.0,-20.0,93.0,94.0,-12.0,92.0,-6.0,-15.0,85.0,91.0,86.0,86.0,-117.0,-762.0,83.0,94.0,75.0,77.0,65.0,91.0,90.0,92.0,-5.0,95.0,84.0,-225.0,90.0,94.0,-14.0,75.0,94.0,91.0,89.0,83.0,86.0,88.0,94.0,21.0,76.0,-233.0,-116.0,93.0,-221.0,96.0,90.0,-180.0,91.0,90.0,27.0,-8.0,-5.0,94.0,92.0,-11.0,89.0,-15.0,88.0,84.0,94.0,-333.0,61.0,87.0,86.0,49.0,90.0,90.0,-15.0,84.0,87.0,91.0,11.0,84.0,93.0,83.0,-20.0,96.0,92.0,94.0,93.0,94.0,92.0,93.0,93.0,92.0,84.0,31.0,91.0,43.0,-242.0


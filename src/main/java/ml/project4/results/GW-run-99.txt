
Running experiments on BasicGridWorld:
Pr(succ) = 0.99
[0,1,0,0]
[0,0,0,0]
[0,1,0,0]
[0,0,0,0]

Max iterations: 100, max intervals: 100
//Value Iteration Analysis//
Value Iteration,688,8,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,8,6,6,6,6,6,6,6,6,6,6,7,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,7,6,6,6,6,7,6,7,6,6,6,6,6,6,8,6,6,6,6,6,6,6,6,6,6,6,6

This is your optimal policy:
[v,*,>,<]
[>,>,^,^]
[^,*,^,^]
[^,>,>,^]

//Policy Iteration Analysis//
Policy Iteration,442,536,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,7,6,7,6,6,6,6,6,6,8,6,6,6,8,7,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,8,6,6,6,6,6,6,6,6,6,6,6,6,8,6,6,6,6

This is your optimal policy:
[v,*,>,v]
[>,>,^,^]
[^,*,^,^]
[^,>,>,^]

//Q Learning Analysis//
Q Learning,24,10,67,17,13,15,8,6,13,18,16,25,17,58,56,46,23,14,11,514,30,11,49,191,17,15,86,17,10,27,48,33,20,31,12,10,14,11,15,31,9,12,21,11,16,9,13,8,12,11,11,74,17,11,18,39,10,13,11,8,8,9,20,73,42,12,14,25,12,12,23,19,9,8,13,15,10,8,16,25,8,13,7,12,10,26,46,11,11,12,16,8,27,11,48,10,12,16,37,18

This is your optimal policy:
[>,*,>,>]
[>,>,>,^]
[^,*,>,^]
[^,<,<,>]

//Aggregate Analysis//

The data below shows the number of passes the agent required to converge
on the optimal policy
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,1,2,3,4,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5
Policy Iteration,1,2,3,4,5,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6
Q Learning,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,2,2,2,2,2,2,2,2,1,2,2,2,2,1,2,2,2

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,688,8,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,8,6,6,6,6,6,6,6,6,6,6,7,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,7,6,6,6,6,7,6,7,6,6,6,6,6,6,8,6,6,6,6,6,6,6,6,6,6,6,6
Policy Iteration,442,536,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,7,6,7,6,6,6,6,6,6,8,6,6,6,8,7,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,8,6,6,6,6,6,6,6,6,6,6,6,6,8,6,6,6,6
Q Learning,24,10,67,17,13,15,8,6,13,18,16,25,17,58,56,46,23,14,11,514,30,11,49,191,17,15,86,17,10,27,48,33,20,31,12,10,14,11,15,31,9,12,21,11,16,9,13,8,12,11,11,74,17,11,18,39,10,13,11,8,8,9,20,73,42,12,14,25,12,12,23,19,9,8,13,15,10,8,16,25,8,13,7,12,10,26,46,11,11,12,16,8,27,11,48,10,12,16,37,18

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,60,3,5,7,8,8,7,18,6,3,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,2,1,1,8,2,2,1,1,1,1,1,1,1,1,1,1,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,5,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,6,1,1,1,1,1,1,1,2,1,1,1
Policy Iteration,3,1,1,2,2,3,3,7,6,4,4,6,6,5,4,4,16,3,2,3,2,2,2,2,2,2,4,2,2,2,3,3,2,2,2,2,3,2,2,2,2,2,2,2,2,10,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,4,4,4,4,4,3,4,4,3,3,4,4,3,3,3,4,3,3,3,4,3
Q Learning,6,1,3,4,3,3,4,16,10,17,4,3,3,5,7,5,4,4,4,7,4,4,2,5,3,5,6,5,2,7,4,6,5,4,2,5,3,4,5,6,5,6,3,15,9,4,7,10,8,4,7,9,7,10,6,4,5,9,7,7,7,4,5,8,10,27,21,23,12,12,22,23,24,13,14,26,5,5,7,7,18,11,9,8,6,17,14,6,12,6,10,9,10,8,12,8,12,18,18,13

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration Rewards,-586.0,94.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,94.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,95.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,95.0,96.0,96.0,96.0,96.0,95.0,96.0,95.0,96.0,96.0,96.0,96.0,96.0,96.0,94.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0
Policy Iteration Rewards,-340.0,-434.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,95.0,96.0,95.0,96.0,96.0,96.0,96.0,96.0,96.0,-5.0,96.0,96.0,96.0,94.0,95.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,94.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,96.0,94.0,96.0,96.0,96.0,96.0
Q Learning Rewards,-120.0,-205.0,35.0,85.0,89.0,-12.0,94.0,96.0,89.0,-114.0,-13.0,77.0,-113.0,-55.0,46.0,56.0,79.0,88.0,-8.0,-1303.0,72.0,91.0,-244.0,-980.0,-14.0,87.0,-182.0,-14.0,-106.0,75.0,54.0,-30.0,82.0,-28.0,-108.0,92.0,88.0,91.0,87.0,-127.0,93.0,90.0,81.0,91.0,86.0,93.0,89.0,94.0,90.0,91.0,91.0,28.0,-14.0,91.0,84.0,63.0,92.0,89.0,91.0,94.0,94.0,93.0,82.0,29.0,60.0,90.0,88.0,77.0,90.0,90.0,79.0,83.0,93.0,94.0,89.0,87.0,92.0,94.0,86.0,77.0,94.0,-10.0,-4.0,90.0,92.0,76.0,56.0,91.0,-8.0,90.0,86.0,94.0,-123.0,91.0,-45.0,92.0,-108.0,-13.0,65.0,84.0


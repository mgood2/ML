
Running experiments on BasicGridWorld:
Pr(succ) = 0.5
[0,1,0,0]
[0,0,0,0]
[0,1,0,0]
[0,0,0,0]

Max iterations: 100, max intervals: 100
//Value Iteration Analysis//
Value Iteration,9,11,27,13,12,10,18,34,6,19,14,18,10,12,32,33,10,11,15,16,16,24,11,11,14,19,29,55,23,26,16,10,29,12,19,21,7,10,14,14,18,15,20,18,14,12,46,37,11,30,8,23,8,14,42,22,13,11,34,12,19,8,13,8,12,17,72,11,48,18,9,13,12,16,11,16,28,12,7,15,10,14,23,16,9,14,19,15,8,6,14,8,14,13,6,15,12,45,13,16

This is your optimal policy:
[v,*,>,>]
[>,>,^,^]
[^,*,^,^]
[^,<,>,^]

//Policy Iteration Analysis//
Policy Iteration,27,16,17,23,13,9,10,9,15,32,34,29,9,16,8,16,13,18,15,10,8,10,18,21,10,17,13,15,18,6,11,15,8,25,13,10,10,23,31,6,22,11,23,13,18,18,16,10,10,20,31,10,22,20,25,17,16,11,19,17,16,10,6,34,13,41,29,13,23,36,14,13,7,8,15,15,19,8,18,15,23,13,16,16,26,25,15,18,11,29,12,12,23,17,17,41,6,11,33,22

This is your optimal policy:
[v,*,>,<]
[>,>,^,^]
[^,*,^,^]
[^,<,>,^]

//Q Learning Analysis//
Q Learning,61,19,35,70,14,6,6,11,44,6,7,15,97,16,22,19,10,106,10,33,13,20,8,14,20,23,52,44,22,10,19,6,6,10,79,7,18,11,11,10,20,10,10,14,9,45,13,13,10,18,13,8,42,7,105,10,9,12,29,26,30,10,8,19,50,35,31,35,6,38,16,6,10,27,12,11,17,27,23,7,12,13,44,33,12,58,9,9,15,38,7,20,16,8,17,6,13,20,13,14

This is your optimal policy:
[v,*,>,>]
[>,>,>,^]
[^,*,v,>]
[^,<,<,<]

//Aggregate Analysis//

The data below shows the number of passes the agent required to converge
on the optimal policy
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28,28
Policy Iteration,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30
Q Learning,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2

The data below shows the number of steps/actions the agent required to reach
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,9,11,27,13,12,10,18,34,6,19,14,18,10,12,32,33,10,11,15,16,16,24,11,11,14,19,29,55,23,26,16,10,29,12,19,21,7,10,14,14,18,15,20,18,14,12,46,37,11,30,8,23,8,14,42,22,13,11,34,12,19,8,13,8,12,17,72,11,48,18,9,13,12,16,11,16,28,12,7,15,10,14,23,16,9,14,19,15,8,6,14,8,14,13,6,15,12,45,13,16
Policy Iteration,27,16,17,23,13,9,10,9,15,32,34,29,9,16,8,16,13,18,15,10,8,10,18,21,10,17,13,15,18,6,11,15,8,25,13,10,10,23,31,6,22,11,23,13,18,18,16,10,10,20,31,10,22,20,25,17,16,11,19,17,16,10,6,34,13,41,29,13,23,36,14,13,7,8,15,15,19,8,18,15,23,13,16,16,26,25,15,18,11,29,12,12,23,17,17,41,6,11,33,22
Q Learning,61,19,35,70,14,6,6,11,44,6,7,15,97,16,22,19,10,106,10,33,13,20,8,14,20,23,52,44,22,10,19,6,6,10,79,7,18,11,11,10,20,10,10,14,9,45,13,13,10,18,13,8,42,7,105,10,9,12,29,26,30,10,8,19,50,35,31,35,6,38,16,6,10,27,12,11,17,27,23,7,12,13,44,33,12,58,9,9,15,38,7,20,16,8,17,6,13,20,13,14

The data below shows the number of milliseconds the algorithm required to generate
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,67,28,20,18,16,10,7,6,7,7,6,5,6,16,5,4,4,4,4,4,4,4,4,11,5,5,5,5,5,5,9,5,6,8,7,5,5,10,5,5,5,5,5,5,5,6,9,9,9,10,13,6,7,5,5,5,5,5,5,5,5,5,5,8,10,5,5,5,6,7,5,5,8,6,5,5,8,6,6,5,9,9,9,9,12,10,9,9,9,9,9,14,5,5,5,5,8,6,5,6
Policy Iteration,2,3,2,2,3,4,4,5,7,8,7,5,7,12,9,6,9,7,8,8,9,16,11,11,11,12,12,11,11,11,14,11,13,13,14,23,19,23,20,23,21,19,20,19,19,19,20,25,10,10,10,10,10,10,10,11,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,18,10,10,10,10,10,10,10,10,13,11,10,10,13,10,10,10,10,10,10,11,10,10,10,10,10,14
Q Learning,6,1,1,2,5,5,12,6,7,5,7,9,10,8,4,15,5,10,15,10,6,12,6,7,5,9,10,19,5,6,6,5,8,6,8,5,13,9,7,6,13,10,14,4,6,7,3,9,4,5,8,8,7,5,6,10,7,6,4,6,6,7,9,7,6,9,9,10,14,9,13,7,11,9,8,9,12,6,10,7,14,10,9,6,8,9,6,9,7,13,11,12,7,19,12,10,18,14,13,14

The data below shows the total reward gained for
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration Rewards,-6.0,91.0,-24.0,89.0,90.0,92.0,84.0,-130.0,96.0,83.0,88.0,84.0,92.0,-108.0,70.0,69.0,92.0,91.0,87.0,86.0,86.0,78.0,91.0,91.0,88.0,-115.0,73.0,47.0,79.0,76.0,86.0,92.0,-26.0,90.0,-115.0,-18.0,-4.0,92.0,88.0,88.0,84.0,-12.0,82.0,84.0,88.0,90.0,56.0,65.0,-8.0,-27.0,94.0,79.0,94.0,-11.0,-138.0,80.0,-10.0,91.0,68.0,90.0,-16.0,94.0,89.0,94.0,90.0,85.0,-366.0,91.0,54.0,84.0,-105.0,89.0,90.0,-13.0,91.0,-112.0,74.0,90.0,-103.0,-111.0,92.0,-11.0,-218.0,86.0,-6.0,88.0,83.0,87.0,94.0,96.0,88.0,94.0,88.0,-10.0,-3.0,-309.0,-108.0,-141.0,-10.0,-13.0
Policy Iteration Rewards,-24.0,-112.0,85.0,79.0,-208.0,93.0,92.0,93.0,87.0,70.0,68.0,73.0,93.0,-13.0,94.0,86.0,89.0,84.0,87.0,92.0,-5.0,92.0,84.0,81.0,92.0,85.0,89.0,87.0,84.0,96.0,91.0,87.0,94.0,77.0,89.0,92.0,92.0,79.0,71.0,96.0,80.0,91.0,-20.0,89.0,-114.0,-114.0,86.0,-106.0,92.0,82.0,71.0,-106.0,80.0,82.0,77.0,85.0,86.0,91.0,-115.0,-14.0,86.0,-7.0,96.0,-229.0,89.0,-335.0,73.0,89.0,-20.0,66.0,88.0,89.0,95.0,94.0,87.0,-12.0,83.0,94.0,84.0,-12.0,79.0,-109.0,86.0,86.0,-23.0,-22.0,-111.0,-15.0,-8.0,73.0,90.0,90.0,-119.0,85.0,85.0,-236.0,-3.0,91.0,-129.0,-118.0
Q Learning Rewards,41.0,-115.0,-230.0,32.0,88.0,-3.0,-3.0,91.0,-41.0,96.0,95.0,87.0,-94.0,86.0,-118.0,83.0,92.0,-4.0,92.0,-30.0,89.0,82.0,94.0,88.0,82.0,-218.0,50.0,-41.0,80.0,92.0,83.0,96.0,96.0,92.0,23.0,-103.0,84.0,91.0,91.0,92.0,82.0,92.0,92.0,88.0,-6.0,-141.0,89.0,-208.0,92.0,84.0,89.0,94.0,-39.0,95.0,-3.0,92.0,-6.0,90.0,73.0,76.0,-27.0,92.0,94.0,83.0,-245.0,67.0,-226.0,67.0,96.0,64.0,-13.0,96.0,92.0,-24.0,-207.0,91.0,85.0,75.0,79.0,-4.0,90.0,89.0,58.0,-129.0,90.0,44.0,93.0,93.0,87.0,64.0,95.0,82.0,-13.0,-5.0,85.0,96.0,89.0,-17.0,89.0,88.0

